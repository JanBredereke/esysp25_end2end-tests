{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a001530",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dir = '/workspace/build'\n",
    "model_name = 'CNV'\n",
    "\n",
    "# Whether to download an example model created by the FINN developers or\n",
    "# use a custom model.onnx\n",
    "download_example_model = False\n",
    "\n",
    "# Whether to show each partial result in Netron\n",
    "enable_netron = True\n",
    "\n",
    "# Choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = 'decoupled'\n",
    "\n",
    "# Folding factors for the layers\n",
    "# Each tuple is (PE, SIMD, in_fifo_depth) for a layer\n",
    "# PE = Parallelization over Outputs\n",
    "# SIMD = Parallelization over inputs\n",
    "# Higher PE/SIMD equals a faster execution but also more FPGA ressource usage\n",
    "folding = [\n",
    "    (16, 3, 128),\n",
    "    (32, 32, 128),\n",
    "    (16, 32, 128),\n",
    "    (16, 32, 128),\n",
    "    (4, 32, 81),\n",
    "    (1, 32, 2),\n",
    "    (1, 4, 2),\n",
    "    (1, 8, 128),\n",
    "    (3, 1, 3),\n",
    "]\n",
    "\n",
    "# The following parameters are only required if deploying on a real board\n",
    "pynq_type = 'Pynq-Z1'\n",
    "pynq_port = 22\n",
    "pynq_username = 'xilinx'\n",
    "pynq_password = 'xilinx'\n",
    "pynq_target_dir = '/home/xilinx/finn_network'\n",
    "\n",
    "# Clock rate in nanoseconds (e.g. 10ns would be 100 Mhz)\n",
    "target_clk_ns = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95d3b6",
   "metadata": {},
   "source": [
    "# Setup Workspace and Helper Functions\n",
    "\n",
    "Before running make sure to execute **ONE** of the following steps:\n",
    "* Put the exported model from Breviates into */workspace/build/* and change the filename to *model.onnx* (Parameter *download_example_model* must be *False*)\n",
    "* Set the parameter *download_example_model* to *True*\n",
    "\n",
    "(*/workspace/build/* is the root of the project repository on your host system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import brevitas.onnx as bo\n",
    "from os import environ, makedirs, path\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.util.test import get_test_model_trained\n",
    "from finn.util.visualization import showSrc\n",
    "from finn.util.basic import make_build_dir\n",
    "from utils.notebook_helpers import eprint, show_in_netron\n",
    "\n",
    "def getModelPath(suffix: str = None):\n",
    "    if suffix:\n",
    "        return str(path.join(build_dir, f'{model_name}_{suffix}.onnx'))\n",
    "    return str(path.join(build_dir, f'{model_name}.onnx'))\n",
    "\n",
    "if download_example_model:\n",
    "    cnv = get_test_model_trained(\"CNV\", 1, 1)\n",
    "    bo.export_finn_onnx(cnv, (1, 3, 32, 32), getModelPath())\n",
    "if not path.isfile(getModelPath()):\n",
    "    eprint(f'The file \"{getModelPath()}\" is missing!')\n",
    "    \n",
    "pynq_ip = environ.get('PYNQ_IP', None)\n",
    "if not pynq_ip:\n",
    "    eprint('The environment variable PYNQ_IP is not set!')\n",
    "    eprint('Deployment on the physical board will NOT work!')\n",
    "    \n",
    "show_in_netron(getModelPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50b71b",
   "metadata": {},
   "source": [
    "# Tidy-Up input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "\n",
    "model = ModelWrapper(getModelPath())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(getModelPath(suffix='tidy'))\n",
    "\n",
    "show_in_netron(getModelPath(suffix='tidy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17ed9b",
   "metadata": {},
   "source": [
    "## Adding Pre- and Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.onnx as bo\n",
    "from finn.util.pytorch import ToTensor\n",
    "#from finn.transformation.insert_topk import InsertTopK\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(getModelPath(suffix='tidy'))\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = getModelPath(suffix='prepoc')\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType.UINT8)\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "#model = model.transform(InsertTopK(k=1))\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(getModelPath(suffix='pre_post'))\n",
    "\n",
    "show_in_netron(getModelPath(suffix='pre_post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40137e83",
   "metadata": {},
   "source": [
    "# Lowering and Streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = ModelWrapper(getModelPath(suffix='pre_post'))\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "#model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(getModelPath(suffix='streamlined'))\n",
    "\n",
    "show_in_netron(getModelPath(suffix='streamlined'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db00f3",
   "metadata": {},
   "source": [
    "## Partitioning, Conversion to HLS Layers and Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac350905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "model = ModelWrapper(getModelPath(suffix='streamlined'))\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "# TopK to LabelSelect\n",
    "#model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(getModelPath(suffix='dataflow_parent'))\n",
    "\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "model = ModelWrapper(dataflow_model_filename)\n",
    "model.save(getModelPath(suffix='dataflow_model'))\n",
    "\n",
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "\n",
    "for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model.save(getModelPath(suffix='folded'))\n",
    "\n",
    "show_in_netron(getModelPath(suffix='folded'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1400ae6",
   "metadata": {},
   "source": [
    "# Hardware Generation\n",
    "\n",
    "**This will take a long time! Expect it to take more than 30 minutes to finish!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18419289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "\n",
    "model = ModelWrapper(getModelPath(suffix='folded'))\n",
    "model = model.transform(ZynqBuild(platform = pynq_type, period_ns = target_clk_ns))\n",
    "model.save(getModelPath(suffix='synth'))\n",
    "\n",
    "show_in_netron(getModelPath(suffix='synth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199111f",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "**This will only work if the physical target board is present under the specified IP!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "if not pynq_ip:\n",
    "    eprint('Pynq IP not configured. This will fail!')\n",
    "\n",
    "# FINN will use ssh to deploy and run the generated accelerator\n",
    "model = ModelWrapper(getModelPath(suffix='synth'))\n",
    "model = model.transform(DeployToPYNQ(pynq_ip, pynq_port, pynq_username, pynq_password, pynq_target_dir))\n",
    "model.save(getModelPath(suffix='pynq_deploy'))\n",
    "\n",
    "target_dir = pynq_target_dir + \"/\" + model.get_metadata_prop(\"pynq_deployment_dir\").split(\"/\")[-1]\n",
    "\n",
    "print(f'Target directory is \"{target_dir}\".')\n",
    "print('It contains the following files:')\n",
    "! sshpass -p {pynq_password} ssh {pynq_username}@{pynq_ip} -p {pynq_port} 'ls -l {target_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103496e6",
   "metadata": {},
   "source": [
    "# Remote Execution\n",
    "\n",
    "**This will only work if the physical target board is present under the specified IP!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "if not pynq_ip:\n",
    "    eprint('Pynq IP not configured. This will fail!')\n",
    "\n",
    "fn = '/workspace/finn/src/finn/qnn-data/cifar10/cifar10-test-data-class3.npz'\n",
    "x = np.load(fn)[\"arr_0\"]\n",
    "x = x.reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "plt.imshow(x)\n",
    "\n",
    "model = ModelWrapper(getModelPath(suffix='pynq_deploy'))\n",
    "iname = model.graph.input[0].name\n",
    "oname = model.graph.output[0].name\n",
    "ishape = model.get_tensor_shape(iname)\n",
    "input_dict = {iname: x.astype(np.float32).reshape(ishape)}\n",
    "ret = execute_onnx(model, input_dict, True)\n",
    "\n",
    "print(f'Result: {ret[oname]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe041af2",
   "metadata": {},
   "source": [
    "# Accuracy Validation\n",
    "\n",
    "\n",
    "**This will only work if the physical target board is present under the specified IP!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6de7c",
   "metadata": {},
   "source": [
    "Install dataset_loading (only required to be executed once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pynq_ip:\n",
    "    eprint('Pynq IP not configured. This will fail!')\n",
    "\n",
    "! sshpass -p {pynq_password} ssh -t {pynq_username}@{pynq_ip} -p {pynq_port} 'echo {pynq_password} | sudo -S pip3 install git+https://github.com/fbcotter/dataset_loading.git@0.0.4#egg=dataset_loading'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7e8d4",
   "metadata": {},
   "source": [
    "Execute generated validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbf4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pynq_ip:\n",
    "    eprint('Pynq IP not configured. This will fail!')\n",
    "\n",
    "! sshpass -p {pynq_password} ssh -t {pynq_username}@{pynq_ip} -p {pynq_port} 'cd {target_dir}; echo {pynq_password} | sudo -S python3.6 validate.py --dataset cifar10 --batchsize 1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d0e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
